# ğŸ“Š ç›‘æ§æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ç›‘æ§ Python å¤šé¡¹ç›®å¹³å°çš„è¿è¡ŒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡ã€‚

## ğŸ“‹ ç›‘æ§æ¦‚è¿°

### ç›‘æ§ç›®æ ‡

- **å¯ç”¨æ€§ç›‘æ§**: ç¡®ä¿æœåŠ¡æ­£å¸¸è¿è¡Œ
- **æ€§èƒ½ç›‘æ§**: ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- **èµ„æºç›‘æ§**: ç›‘æ§ CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œä½¿ç”¨
- **ä¸šåŠ¡ç›‘æ§**: ç›‘æ§å…³é”®ä¸šåŠ¡æŒ‡æ ‡
- **å®‰å…¨ç›‘æ§**: ç›‘æ§å®‰å…¨äº‹ä»¶å’Œå¼‚å¸¸

### ç›‘æ§æ¶æ„

```
[åº”ç”¨æœåŠ¡] â†’ [æŒ‡æ ‡æ”¶é›†] â†’ [ç›‘æ§ç³»ç»Ÿ] â†’ [å‘Šè­¦é€šçŸ¥]
     â†“              â†“           â†“           â†“
[æ—¥å¿—è¾“å‡º] â†’ [æ—¥å¿—èšåˆ] â†’ [å¯è§†åŒ–] â†’ [é‚®ä»¶/çŸ­ä¿¡/é’‰é’‰]
```

## ğŸ› ï¸ ç›‘æ§å·¥å…·

### ç³»ç»Ÿç›‘æ§

- **Prometheus**: æŒ‡æ ‡æ”¶é›†å’Œå­˜å‚¨
- **Grafana**: æ•°æ®å¯è§†åŒ–å’Œå‘Šè­¦
- **Node Exporter**: ç³»ç»ŸæŒ‡æ ‡å¯¼å‡º
- **cAdvisor**: å®¹å™¨æŒ‡æ ‡ç›‘æ§

### åº”ç”¨ç›‘æ§

- **è‡ªå®šä¹‰æŒ‡æ ‡**: åº”ç”¨ä¸šåŠ¡æŒ‡æ ‡
- **å¥åº·æ£€æŸ¥**: æœåŠ¡å¥åº·çŠ¶æ€
- **æ€§èƒ½åˆ†æ**: å“åº”æ—¶é—´å’Œååé‡
- **é”™è¯¯è¿½è¸ª**: å¼‚å¸¸å’Œé”™è¯¯ç›‘æ§

### æ—¥å¿—ç›‘æ§

- **ELK Stack**: Elasticsearch + Logstash + Kibana
- **Fluentd**: æ—¥å¿—æ”¶é›†å’Œè½¬å‘
- **Graylog**: æ—¥å¿—ç®¡ç†å’Œåˆ†æ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ç›‘æ§å·¥å…·

```bash
# åˆ›å»ºç›‘æ§ç›®å½•
mkdir -p monitoring
cd monitoring

# ä¸‹è½½ Prometheus
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar -xzf prometheus-2.45.0.linux-amd64.tar.gz
cd prometheus-2.45.0

# ä¸‹è½½ Node Exporter
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
tar -xzf node_exporter-1.6.1.linux-amd64.tar.gz
```

### Docker Compose éƒ¨ç½²

```yaml
# monitoring/docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped

  # cAdvisor
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
```

## ğŸ”§ é…ç½®ç›‘æ§

### Prometheus é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  # ç›‘æ§ Prometheus è‡ªèº«
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # ç›‘æ§ Node Exporter
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  # ç›‘æ§ cAdvisor
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  # ç›‘æ§åº”ç”¨æœåŠ¡
  - job_name: 'myplatform-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # ç›‘æ§ Redis
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  # ç›‘æ§ MySQL
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql:3306']
```

### å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# monitoring/alert_rules.yml
groups:
  - name: system_alerts
    rules:
      # CPU ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% for more than 5 minutes"

      # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% for more than 5 minutes"

      # ç£ç›˜ä½¿ç”¨ç‡å‘Šè­¦
      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 90% for more than 5 minutes"

  - name: application_alerts
    rules:
      # åº”ç”¨å“åº”æ—¶é—´å‘Šè­¦
      - alert: HighResponseTime
        expr: http_request_duration_seconds > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response time on {{ $labels.instance }}"
          description: "Response time is above 2 seconds"

      # é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate on {{ $labels.instance }}"
          description: "Error rate is above 5% for more than 5 minutes"

  - name: service_alerts
    rules:
      # Redis è¿æ¥å‘Šè­¦
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down on {{ $labels.instance }}"
          description: "Redis service is not responding"

      # MySQL è¿æ¥å‘Šè­¦
      - alert: MySQLDown
        expr: mysql_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MySQL is down on {{ $labels.instance }}"
          description: "MySQL service is not responding"
```

## ğŸ“Š åº”ç”¨æŒ‡æ ‡æ”¶é›†

### è‡ªå®šä¹‰æŒ‡æ ‡

```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Summary
from prometheus_client.exposition import start_http_server
import time

# è¯·æ±‚è®¡æ•°å™¨
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

# è¯·æ±‚æŒç»­æ—¶é—´
REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

# æ´»è·ƒè¿æ¥æ•°
ACTIVE_CONNECTIONS = Gauge(
    'http_active_connections',
    'Number of active HTTP connections'
)

# ä¸šåŠ¡æŒ‡æ ‡
USER_REGISTRATIONS = Counter(
    'user_registrations_total',
    'Total user registrations'
)

TRADE_VOLUME = Counter(
    'trade_volume_total',
    'Total trading volume',
    ['symbol', 'side']
)

# æ€§èƒ½æŒ‡æ ‡
PROCESSING_TIME = Summary(
    'data_processing_seconds',
    'Time spent processing data'
)

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
def start_metrics_server(port=8000):
    """å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨"""
    start_http_server(port)
    print(f"Metrics server started on port {port}")

# ä¸­é—´ä»¶è£…é¥°å™¨
def track_metrics(func):
    """è·Ÿè¸ªè¯·æ±‚æŒ‡æ ‡çš„è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            REQUEST_COUNT.labels(
                method='GET',
                endpoint=func.__name__,
                status=200
            ).inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(
                method='GET',
                endpoint=func.__name__,
                status=500
            ).inc()
            raise e
        finally:
            duration = time.time() - start_time
            REQUEST_DURATION.labels(
                method='GET',
                endpoint=func.__name__
            ).observe(duration)
    
    return wrapper
```

### å¥åº·æ£€æŸ¥ç«¯ç‚¹

```python
# monitoring/health_check.py
from flask import Flask, jsonify
from common.storage.redis_client import RedisClient
from common.storage.db_client import DatabaseManager
import psutil
import time

app = Flask(__name__)

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.redis_client = RedisClient()
        self.db_manager = DatabaseManager()
    
    def check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                "status": "healthy",
                "timestamp": time.time(),
                "system": {
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "disk_percent": disk.percent
                }
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "timestamp": time.time(),
                "error": str(e)
            }
    
    def check_redis_health(self):
        """æ£€æŸ¥ Redis å¥åº·çŠ¶æ€"""
        try:
            # ç®€å•çš„ ping æµ‹è¯•
            self.redis_client.ping()
            return {"status": "healthy", "service": "redis"}
        except Exception as e:
            return {"status": "unhealthy", "service": "redis", "error": str(e)}
    
    def check_database_health(self):
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        try:
            self.db_manager.ping()
            return {"status": "healthy", "service": "database"}
        except Exception as e:
            return {"status": "unhealthy", "service": "database", "error": str(e)}
    
    def comprehensive_health_check(self):
        """ç»¼åˆå¥åº·æ£€æŸ¥"""
        system_health = self.check_system_health()
        redis_health = self.check_redis_health()
        db_health = self.check_database_health()
        
        overall_status = "healthy"
        if any(check["status"] == "unhealthy" for check in [system_health, redis_health, db_health]):
            overall_status = "unhealthy"
        
        return {
            "status": overall_status,
            "timestamp": time.time(),
            "checks": {
                "system": system_health,
                "redis": redis_health,
                "database": db_health
            }
        }

health_checker = HealthChecker()

@app.route('/health')
def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify(health_checker.comprehensive_health_check())

@app.route('/health/ready')
def ready():
    """å°±ç»ªæ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "ready"})

@app.route('/health/live')
def live():
    """å­˜æ´»æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "alive"})

@app.route('/metrics')
def metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    # è¿™é‡Œåº”è¯¥è¿”å› Prometheus æ ¼å¼çš„æŒ‡æ ‡
    # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨ prometheus_client
    return "metrics endpoint"
```

## ğŸ“ˆ Grafana ä»ªè¡¨æ¿

### ç³»ç»Ÿç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "System Monitoring",
    "panels": [
      {
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU %"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory %"
          }
        ]
      },
      {
        "title": "Disk Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100",
            "legendFormat": "Disk %"
          }
        ]
      },
      {
        "title": "Network Traffic",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(node_network_receive_bytes_total[5m])",
            "legendFormat": "Receive"
          },
          {
            "expr": "rate(node_network_transmit_bytes_total[5m])",
            "legendFormat": "Transmit"
          }
        ]
      }
    ]
  }
}
```

### åº”ç”¨ç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Application Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx errors"
          },
          {
            "expr": "rate(http_requests_total{status=~\"4..\"}[5m])",
            "legendFormat": "4xx errors"
          }
        ]
      },
      {
        "title": "Active Connections",
        "type": "stat",
        "targets": [
          {
            "expr": "http_active_connections",
            "legendFormat": "Active Connections"
          }
        ]
      }
    ]
  }
}
```

## ğŸ”” å‘Šè­¦é€šçŸ¥

### å‘Šè­¦ç®¡ç†å™¨é…ç½®

```yaml
# monitoring/alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://127.0.0.1:5001/'

  - name: 'slack'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']
```

### å‘Šè­¦é€šçŸ¥å®ç°

```python
# monitoring/alert_notifier.py
import smtplib
import requests
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from common.logging.logger import get_logger

logger = get_logger(__name__)

class AlertNotifier:
    """å‘Šè­¦é€šçŸ¥å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logger
    
    def send_email_alert(self, alert):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.config['email']['from']
            msg['To'] = self.config['email']['to']
            msg['Subject'] = f"Alert: {alert['summary']}"
            
            body = f"""
            Alert Details:
            - Summary: {alert['summary']}
            - Description: {alert['description']}
            - Severity: {alert['severity']}
            - Instance: {alert['instance']}
            - Time: {alert['timestamp']}
            """
            
            msg.attach(MIMEText(body, 'plain'))
            
            server = smtplib.SMTP(self.config['email']['smtp_server'])
            server.starttls()
            server.login(self.config['email']['username'], self.config['email']['password'])
            server.send_message(msg)
            server.quit()
            
            self.logger.info(f"Email alert sent: {alert['summary']}")
            
        except Exception as e:
            self.logger.error(f"Failed to send email alert: {e}")
    
    def send_slack_alert(self, alert):
        """å‘é€ Slack å‘Šè­¦"""
        try:
            webhook_url = self.config['slack']['webhook_url']
            
            message = {
                "text": f"ğŸš¨ *Alert: {alert['summary']}*",
                "attachments": [
                    {
                        "color": "danger" if alert['severity'] == 'critical' else "warning",
                        "fields": [
                            {
                                "title": "Description",
                                "value": alert['description'],
                                "short": False
                            },
                            {
                                "title": "Severity",
                                "value": alert['severity'],
                                "short": True
                            },
                            {
                                "title": "Instance",
                                "value": alert['instance'],
                                "short": True
                            }
                        ]
                    }
                ]
            }
            
            response = requests.post(webhook_url, json=message)
            response.raise_for_status()
            
            self.logger.info(f"Slack alert sent: {alert['summary']}")
            
        except Exception as e:
            self.logger.error(f"Failed to send Slack alert: {e}")
    
    def send_dingtalk_alert(self, alert):
        """å‘é€é’‰é’‰å‘Šè­¦"""
        try:
            webhook_url = self.config['dingtalk']['webhook_url']
            
            message = {
                "msgtype": "markdown",
                "markdown": {
                    "title": f"ğŸš¨ å‘Šè­¦: {alert['summary']}",
                    "text": f"""
                    ### ğŸš¨ ç³»ç»Ÿå‘Šè­¦
                    
                    **æè¿°**: {alert['description']}
                    
                    **ä¸¥é‡ç¨‹åº¦**: {alert['severity']}
                    
                    **å®ä¾‹**: {alert['instance']}
                    
                    **æ—¶é—´**: {alert['timestamp']}
                    """
                }
            }
            
            response = requests.post(webhook_url, json=message)
            response.raise_for_status()
            
            self.logger.info(f"DingTalk alert sent: {alert['summary']}")
            
        except Exception as e:
            self.logger.error(f"Failed to send DingTalk alert: {e}")
    
    def notify(self, alert):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        # æ ¹æ®é…ç½®å‘é€ä¸åŒç±»å‹çš„é€šçŸ¥
        if self.config.get('email', {}).get('enabled', False):
            self.send_email_alert(alert)
        
        if self.config.get('slack', {}).get('enabled', False):
            self.send_slack_alert(alert)
        
        if self.config.get('dingtalk', {}).get('enabled', False):
            self.send_dingtalk_alert(alert)
```

## ğŸ“Š æ—¥å¿—ç›‘æ§

### æ—¥å¿—æ”¶é›†é…ç½®

```yaml
# monitoring/fluentd.conf
<source>
  @type tail
  path /var/log/app/*.log
  pos_file /var/log/fluentd/app.log.pos
  tag app.logs
  format json
  time_key timestamp
</source>

<filter app.logs>
  @type parser
  key_name message
  <parse>
    @type regexp
    expression /^(?<level>\w+)\s+(?<message>.*)$/
  </parse>
</filter>

<match app.logs>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name app-logs
  type_name _doc
  logstash_format true
  logstash_prefix app-logs
  <buffer>
    @type file
    path /var/log/fluentd/buffer
    flush_interval 5s
    chunk_limit_size 2M
    queue_limit_length 8
  </buffer>
</match>
```

### æ—¥å¿—åˆ†ææŸ¥è¯¢

```python
# monitoring/log_analyzer.py
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta
import json

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, es_host='localhost', es_port=9200):
        self.es = Elasticsearch([{'host': es_host, 'port': es_port}])
    
    def get_error_logs(self, hours=24):
        """è·å–é”™è¯¯æ—¥å¿—"""
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"level": "ERROR"}},
                        {"range": {
                            "@timestamp": {
                                "gte": f"now-{hours}h",
                                "lte": "now"
                            }
                        }}
                    ]
                }
            },
            "sort": [{"@timestamp": {"order": "desc"}}],
            "size": 100
        }
        
        response = self.es.search(index="app-logs", body=query)
        return response['hits']['hits']
    
    def get_error_summary(self, hours=24):
        """è·å–é”™è¯¯æ‘˜è¦"""
        query = {
            "query": {
                "range": {
                    "@timestamp": {
                        "gte": f"now-{hours}h",
                        "lte": "now"
                    }
                }
            },
            "aggs": {
                "error_levels": {
                    "terms": {"field": "level.keyword"}
                },
                "error_timeline": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "interval": "1h"
                    }
                }
            }
        }
        
        response = self.es.search(index="app-logs", body=query)
        return response['aggregations']
    
    def search_logs(self, query_text, hours=24):
        """æœç´¢æ—¥å¿—"""
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"query_string": {"query": query_text}},
                        {"range": {
                            "@timestamp": {
                                "gte": f"now-{hours}h",
                                "lte": "now"
                            }
                        }}
                    ]
                }
            },
            "sort": [{"@timestamp": {"order": "desc"}}],
            "size": 100
        }
        
        response = self.es.search(index="app-logs", body=query)
        return response['hits']['hits']
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§ç›‘æ§é—®é¢˜

#### 1. Prometheus æ— æ³•æŠ“å–æŒ‡æ ‡

```bash
# æ£€æŸ¥ç›®æ ‡çŠ¶æ€
curl http://localhost:9090/api/v1/targets

# æ£€æŸ¥é…ç½®
curl http://localhost:9090/api/v1/status/config

# æŸ¥çœ‹æ—¥å¿—
docker logs prometheus
```

#### 2. Grafana æ— æ³•è¿æ¥æ•°æ®æº

```bash
# æ£€æŸ¥ Prometheus è¿æ¥
curl http://prometheus:9090/api/v1/query?query=up

# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec grafana ping prometheus

# æŸ¥çœ‹ Grafana æ—¥å¿—
docker logs grafana
```

#### 3. å‘Šè­¦ä¸è§¦å‘

```bash
# æ£€æŸ¥å‘Šè­¦è§„åˆ™
curl http://localhost:9090/api/v1/rules

# æ£€æŸ¥å‘Šè­¦çŠ¶æ€
curl http://localhost:9090/api/v1/alerts

# æ£€æŸ¥å‘Šè­¦ç®¡ç†å™¨
curl http://localhost:9093/api/v1/alerts
```

### æ€§èƒ½ä¼˜åŒ–

```python
# æŒ‡æ ‡æ”¶é›†ä¼˜åŒ–
import time
from functools import wraps

def measure_time(func):
    """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        PROCESSING_TIME.observe(duration)
        
        return result
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@measure_time
def process_data(data):
    """å¤„ç†æ•°æ®"""
    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    return data.upper()
```

## ğŸ“š ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ [ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²](production.md)
- äº†è§£ [Docker éƒ¨ç½²æŒ‡å—](docker.md)
- é˜…è¯» [å¿«é€Ÿå¼€å§‹](../getting-started/quickstart.md)

## ğŸ¤ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°ç›‘æ§é—®é¢˜ï¼š

1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
2. æŸ¥çœ‹é”™è¯¯æ—¥å¿—
3. éªŒè¯é…ç½®æ–‡ä»¶
4. è”ç³»æŠ€æœ¯æ”¯æŒ
